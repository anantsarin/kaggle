{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-27T06:34:52.098647Z","iopub.execute_input":"2022-07-27T06:34:52.099194Z","iopub.status.idle":"2022-07-27T06:34:52.145731Z","shell.execute_reply.started":"2022-07-27T06:34:52.099091Z","shell.execute_reply":"2022-07-27T06:34:52.144148Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/sampleSubmission.csv\n/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read data from files \ntrain = pd.read_csv( \"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\ntest = pd.read_csv( \"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv( \"/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint (\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size,test[\"review\"].size, unlabeled_train[\"review\"].size ))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:34:52.148476Z","iopub.execute_input":"2022-07-27T06:34:52.149207Z","iopub.status.idle":"2022-07-27T06:34:55.394118Z","shell.execute_reply.started":"2022-07-27T06:34:52.149169Z","shell.execute_reply":"2022-07-27T06:34:55.392744Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import various modules for string cleaning\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:34:55.398933Z","iopub.execute_input":"2022-07-27T06:34:55.399394Z","iopub.status.idle":"2022-07-27T06:34:56.849316Z","shell.execute_reply.started":"2022-07-27T06:34:55.399342Z","shell.execute_reply":"2022-07-27T06:34:56.848080Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Download the punkt tokenizer for sentence splitting\nimport nltk.data\n# nltk.download()   \n\n# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    #\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:34:56.851171Z","iopub.execute_input":"2022-07-27T06:34:56.851945Z","iopub.status.idle":"2022-07-27T06:34:56.876885Z","shell.execute_reply.started":"2022-07-27T06:34:56.851888Z","shell.execute_reply":"2022-07-27T06:34:56.875604Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\nprint (\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint (\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:34:56.879433Z","iopub.execute_input":"2022-07-27T06:34:56.879793Z","iopub.status.idle":"2022-07-27T06:38:58.155064Z","shell.execute_reply.started":"2022-07-27T06:34:56.879758Z","shell.execute_reply":"2022-07-27T06:38:58.153671Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Parsing sentences from training set\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  MarkupResemblesLocatorWarning\n/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:408: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n  MarkupResemblesLocatorWarning\n","output_type":"stream"},{"name":"stdout","text":"Parsing sentences from unlabeled set\n","output_type":"stream"}]},{"cell_type":"code","source":"print (len(sentences))\nprint (sentences[0])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:38:58.156856Z","iopub.execute_input":"2022-07-27T06:38:58.157522Z","iopub.status.idle":"2022-07-27T06:38:58.163458Z","shell.execute_reply.started":"2022-07-27T06:38:58.157482Z","shell.execute_reply":"2022-07-27T06:38:58.162191Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"794002\n['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec \n# creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint (\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:38:58.165619Z","iopub.execute_input":"2022-07-27T06:38:58.166720Z","iopub.status.idle":"2022-07-27T06:41:19.610860Z","shell.execute_reply.started":"2022-07-27T06:38:58.166658Z","shell.execute_reply":"2022-07-27T06:41:19.609327Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n","output_type":"stream"}]},{"cell_type":"code","source":"model.wv.most_similar(\"man\")","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:46:22.224724Z","iopub.execute_input":"2022-07-27T06:46:22.225197Z","iopub.status.idle":"2022-07-27T06:46:22.256255Z","shell.execute_reply.started":"2022-07-27T06:46:22.225161Z","shell.execute_reply":"2022-07-27T06:46:22.253815Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[('woman', 0.6139178276062012),\n ('lady', 0.5803261995315552),\n ('lad', 0.5547842979431152),\n ('millionaire', 0.5340404510498047),\n ('monk', 0.5259341597557068),\n ('farmer', 0.5229904651641846),\n ('soldier', 0.5208258628845215),\n ('chap', 0.5045815706253052),\n ('men', 0.5028882622718811),\n ('businessman', 0.5026947259902954)]"},"metadata":{}}]}]}