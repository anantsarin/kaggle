{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-27T06:13:18.490661Z","iopub.execute_input":"2022-07-27T06:13:18.491087Z","iopub.status.idle":"2022-07-27T06:13:18.501736Z","shell.execute_reply.started":"2022-07-27T06:13:18.491054Z","shell.execute_reply":"2022-07-27T06:13:18.500533Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/sampleSubmission.csv\n/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read data from files \ntrain = pd.read_csv( \"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\ntest = pd.read_csv( \"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv( \"/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint (\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size,test[\"review\"].size, unlabeled_train[\"review\"].size ))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:16:26.419015Z","iopub.execute_input":"2022-07-27T06:16:26.419444Z","iopub.status.idle":"2022-07-27T06:16:28.896474Z","shell.execute_reply.started":"2022-07-27T06:16:26.419412Z","shell.execute_reply":"2022-07-27T06:16:28.895122Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import various modules for string cleaning\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:16:53.159393Z","iopub.execute_input":"2022-07-27T06:16:53.159846Z","iopub.status.idle":"2022-07-27T06:16:54.724799Z","shell.execute_reply.started":"2022-07-27T06:16:53.159814Z","shell.execute_reply":"2022-07-27T06:16:54.723380Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Download the punkt tokenizer for sentence splitting\nimport nltk.data\n# nltk.download()   \n\n# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    #\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:17:17.883975Z","iopub.execute_input":"2022-07-27T06:17:17.884426Z","iopub.status.idle":"2022-07-27T06:17:17.912470Z","shell.execute_reply.started":"2022-07-27T06:17:17.884391Z","shell.execute_reply":"2022-07-27T06:17:17.911483Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\nprint (\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint (\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:17:54.998476Z","iopub.execute_input":"2022-07-27T06:17:54.998933Z","iopub.status.idle":"2022-07-27T06:22:08.766809Z","shell.execute_reply.started":"2022-07-27T06:17:54.998899Z","shell.execute_reply":"2022-07-27T06:22:08.765860Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Parsing sentences from training set\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  MarkupResemblesLocatorWarning\n/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:408: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n  MarkupResemblesLocatorWarning\n","output_type":"stream"},{"name":"stdout","text":"Parsing sentences from unlabeled set\n","output_type":"stream"}]},{"cell_type":"code","source":"print (len(sentences))\nprint (sentences[0])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:22:49.873208Z","iopub.execute_input":"2022-07-27T06:22:49.873629Z","iopub.status.idle":"2022-07-27T06:22:49.879970Z","shell.execute_reply.started":"2022-07-27T06:22:49.873600Z","shell.execute_reply":"2022-07-27T06:22:49.879095Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"794002\n['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec \n# creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint (\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:22:59.339240Z","iopub.execute_input":"2022-07-27T06:22:59.339605Z","iopub.status.idle":"2022-07-27T06:22:59.779721Z","shell.execute_reply.started":"2022-07-27T06:22:59.339576Z","shell.execute_reply":"2022-07-27T06:22:59.778123Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1736014978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# If you don't plan to train the model any further, calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"],"ename":"TypeError","evalue":"__init__() got an unexpected keyword argument 'size'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}